{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eafc09d",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "In this notebook we would talk about the dimensionality reduction techniques.\n",
    "\n",
    "As the data gets more complex, it usually tends to have higher number of features (or dimensions) to it. So, it is desirable to reduce them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6614c13",
   "metadata": {},
   "source": [
    "# Why Dimensionality Reduction?\n",
    "\n",
    "* Helps reduce the dimensionality curse\n",
    "* Space required to store the data is reduced as the number of dimensions comes down\n",
    "* Less dimensions lead to less computation/training time\n",
    "* Some algorithms do not perform well when we have a large dimensions. So reducing these dimensions needs to happen for the algorithm to be useful\n",
    "* It takes care of multicollinearity by removing redundant features. For example, you have two variables – ‘time spent on treadmill in minutes’ and ‘calories burnt’. These variables are highly correlated as the more time you spend running on a treadmill, the more calories you will burn. Hence, there is no point in storing both as just one of them does what you require\n",
    "* It helps in visualizing data. As discussed earlier, it is very difficult to visualize data in higher dimensions so reducing our space to 2D or 3D may allow us to plot and observe patterns more clearly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcffb961",
   "metadata": {},
   "source": [
    "# Types of Dimensionality Reduction\n",
    "\n",
    "There are mainly two types of dimensionality reduction methods. Both methods reduce the number of dimensions but in different ways. It is very important to distinguish between those two types of methods. \n",
    "\n",
    "One type of method only keeps the most important features in the dataset and removes the redundant features. There is no transformation applied to the set of features. Backward elimination, Forward selection and Random forests are examples of this method. \n",
    "\n",
    "The other method finds a combination of new features. An appropriate transformation is applied to the set of features. The new set of features contains different values instead of the original values. This method can be further divided into Linear methods and Non-linear methods. Non-linear methods are well known as Manifold learning. Principal Component Analysis (PCA), Factor Analysis (FA), Linear Discriminant Analysis (LDA) and Truncated Singular Value Decomposition (SVD) are examples of linear dimensionality reduction methods. Kernel PCA, t-distributed Stochastic Neighbor Embedding (t-SNE), Multidimensional Scaling (MDS) and Isometric mapping (Isomap) are examples of non-linear dimensionality reduction methods.\n",
    "\n",
    "Now, let's start discussing some of the methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10308daa",
   "metadata": {},
   "source": [
    "# Missing Value Ratio\n",
    "\n",
    "Suppose you’re given a dataset. What would be your first step? You would naturally want to explore the data first before building model. While exploring the data, you find that your dataset has some missing values. Now what? You will try to find out the reason for these missing values and then impute them or drop the variables entirely which have missing values (using appropriate methods).\n",
    "\n",
    "What if we have too many missing values (say more than 50%)? Should we impute the missing values or drop the variable? I would prefer to drop the variable since it will not have much information. However, this isn’t set in stone. We can set a threshold value and if the percentage of missing values in any variable is more than that threshold, we will drop the variable. This is usually powered by the domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54273034",
   "metadata": {},
   "source": [
    "# Low Variance Filter\n",
    "\n",
    "Consider a variable in our dataset where all the observations have the same value, say 1. If we use this variable, do you think it can improve the model we will build? The answer is no, because this variable will have zero variance.\n",
    "\n",
    "So, we need to calculate the variance of each variable we are given. Then drop the variables having low variance as compared to other variables in our dataset. The reason for doing this, as I mentioned above, is that variables with a low variance will not affect the target variable.\n",
    "\n",
    "Do take care for categorical variables though. They would most likely need more attention and you should use counts of the unique values and other metrics for them to determine if the variable is adding any useful value or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d37e2",
   "metadata": {},
   "source": [
    "# High Correlation filter\n",
    "\n",
    "High correlation between two variables means they have similar trends and are likely to carry similar information. This can bring down the performance of some models drastically (linear and logistic regression models, for instance). We can calculate the correlation between independent numerical variables that are numerical in nature. If the correlation coefficient crosses a certain threshold value, we can drop one of the variables (dropping a variable is highly subjective and should always be done keeping the domain in mind).\n",
    "\n",
    "As a general guideline, we should keep those variables which show a decent or high correlation with the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e468f",
   "metadata": {},
   "source": [
    "# Random Forest \n",
    "\n",
    "Random Forest is one of the most widely used algorithms for feature selection. It comes packaged with in-built feature importance so you don’t need to program that separately. This helps us select a smaller subset of features. \n",
    "\n",
    "Although it's a great way to select features, beware of some important things you need to be wary of:\n",
    "\n",
    "* **They are completely useless if your model is weak.** If your model does not generalize to validation data - like in the case you mentioned of completely random predictors - then feature importances have no meaning. That is because all the splits are simply overfitting the training data and not capturing any real trend, so all the gini impurity you sum is useless\n",
    "\n",
    "* **They are strongly influenced by correlated features.** It is a fact. Just know it and perform some good old feature engineering before hand to avoid having features that are too correlated\n",
    "\n",
    "* **They are biased towards numerical and high cardinality features.** This is definitely a problem. There are some alternative approaches to help relieve this\n",
    "\n",
    "However, given that you've kept those things in mind, it can be a great way to select features.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(random_state=1, max_depth=10)\n",
    "df=pd.get_dummies(df)\n",
    "model.fit(df, train.target_variable)\n",
    "features = df.columns\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[-9:]  # top 10 features\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b95786",
   "metadata": {},
   "source": [
    "# Permutation Importance\n",
    "\n",
    "Permutation feature importance is a model inspection technique that can be used for any fitted estimator when the data is tabular. This is especially useful for non-linear or opaque estimators. The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. This technique benefits from being model agnostic and can be calculated many times with different permutations of the feature.\n",
    "\n",
    "**Important:** Features that are deemed of **low importance for a bad model** (low cross-validation score) could be **very important for a good model.** Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances. Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is for a particular model.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "```python\n",
    ">>> clf = LogisticRegression().fit(X, y)\n",
    ">>> result = permutation_importance(clf, X, y, n_repeats=10,\n",
    "...                                 random_state=0)\n",
    ">>> result.importances_mean\n",
    "array([0.4666..., 0.       , 0.       ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c49fe",
   "metadata": {},
   "source": [
    "# Backward Feature Elimination\n",
    "\n",
    "Follow the below steps to understand and use the ‘Backward Feature Elimination’ technique:\n",
    "\n",
    "* We first take all the n variables present in our dataset and train the model using them\n",
    "* We then calculate the performance of the model\n",
    "* Now, we compute the performance of the model after eliminating each variable (n times), i.e., we drop one variable every time and train the model on the remaining n-1 variables\n",
    "* We identify the variable whose removal has produced the smallest (or no) change in the performance of the model, and then drop that variable\n",
    "* Repeat this process until no variable can be dropped\n",
    "\n",
    "**Important:** Just like the Permutation Importance, features deemed **low importance for bad model** could be **very important for good model.** Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances. This method does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is for a particular model.\n",
    "\n",
    "Also, it tends to be slow since a model is fit multiple times. You need to take training time and stuff into account.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "```python\n",
    ">>> estimator = SVR(kernel=\"linear\")\n",
    ">>> selector = RFE(estimator, n_features_to_select=5, step=1)\n",
    ">>> selector = selector.fit(X, y)\n",
    ">>> selector.support_\n",
    "array([ True,  True,  True,  True,  True, False, False, False, False,\n",
    "       False])\n",
    ">>> selector.ranking_\n",
    "array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6d6617",
   "metadata": {},
   "source": [
    "# Forward Feature Selection\n",
    "\n",
    "This is the opposite process of the Backward Feature Elimination we saw above. Instead of eliminating features, we try to find the best features which improve the performance of the model. This technique works as follows:\n",
    "\n",
    "* We start with a single feature. Essentially, we train the model n number of times using each feature separately\n",
    "* The variable giving the best performance is selected as the starting variable\n",
    "* Then we repeat this process and add one variable at a time. The variable that produces the highest increase in performance is retained\n",
    "* We repeat this process until no significant improvement is seen in the model’s performance\n",
    "\n",
    "**Important:** Just like the Permutation Importance, features deemed **low importance for bad model** could be **very important for good model.** Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances. This method does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is for a particular model.\n",
    "\n",
    "Also, it tends to be slow since a model is fit multiple times. You need to take training time and stuff into account.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "```python\n",
    ">>> knn = KNeighborsClassifier(n_neighbors=3)\n",
    ">>> sfs = SequentialFeatureSelector(knn, n_features_to_select=3)\n",
    ">>> sfs.fit(X, y)\n",
    "SequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3),\n",
    "                          n_features_to_select=3)\n",
    ">>> sfs.get_support()\n",
    "array([ True, False,  True,  True])\n",
    ">>> sfs.transform(X).shape  # .transform reduces the X\n",
    "(150, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf37a7",
   "metadata": {},
   "source": [
    "# Principal Component Analysis or PCA\n",
    "\n",
    "PCA is a technique which helps us in extracting a new set of variables from an existing large set of variables. These newly extracted variables are called Principal Components. You can refer to this article to learn more about PCA. For your quick reference, below are some of the key points you should know about PCA before proceeding further:\n",
    "\n",
    "* A principal component is a linear combination of the original variables\n",
    "* Principal components are extracted in such a way that the first principal component explains maximum variance in the dataset\n",
    "* Second principal component tries to explain the remaining variance in the dataset and is uncorrelated to the first principal component\n",
    "* Third principal component tries to explain the variance which is not explained by the first two principal components and so on\n",
    "\n",
    "## Important Things to Consider\n",
    "\n",
    "* Make sure to **standardize the variables** before running PCA since it's very sensitive to variance within the components.\n",
    "* Do **not use on categorical variables** since the distance metric used for it won't make sense on categorical variables. \n",
    "* PCA makes the reduced data you get **not really interpretable** compared to the ones you have before.\n",
    "* PCA is still quite **susceptible to throwing strong classification signals away**. PCA geometrically project a data set onto fewer dimensions, where the new variables are called principal components. This is done in such a way that the principal components are orthogonal and have the largest possible variances. So, if the variable is such that above the line it's one class (target variable) and below another, PCA would give up the result. Here's an example: <img src='https://miro.medium.com/max/875/1*F9jEYN5okmKNxjsodQgIow.png'/><br> Although this doesn't happen that much in real life.\n",
    "* If variables are correlated non-linearly, you would lose them.\n",
    "\n",
    "## Example\n",
    "\n",
    "Here's an example:\n",
    "```python\n",
    ">>> pca = PCA(n_components=2)\n",
    ">>> pca.fit(X)\n",
    "PCA(n_components=2)\n",
    ">>> print(pca.explained_variance_ratio_)\n",
    "[0.9924... 0.0075...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c750d",
   "metadata": {},
   "source": [
    "# Manifold Learning: Isomap\n",
    "\n",
    "While PCA is a great dimensionality reduction algorithm, it only takes linear components. This would cause it to lose any non-linear relationships between variables. A set of techniques that take those things into account are called Manifold Learning. And Isomap is one of the standard ones. \n",
    "\n",
    "It uses the geodesic distance between two points:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/716/1*txQ3bGq0SsFgWnxpjgZsaw.png\"/>\n",
    "\n",
    "It is a manifold learning algorithm which tries to preserve the geodesic distance between samples while reducing the dimension. It computes them by finding the neighbours of the distance. \n",
    "\n",
    "**Limitations:** Although it captures a lot more information, it has **problems with manifolds with holes.** Also it is **computationally expensive.**\n",
    "\n",
    "Here's an example: \n",
    "\n",
    "```python\n",
    ">>> X, _ = load_digits(return_X_y=True)\n",
    ">>> X.shape\n",
    "(1797, 64)\n",
    ">>> embedding = Isomap(n_components=2, n_jobs=-1)  # n_jobs=-1 takes all cores to speed it up\n",
    ">>> X_transformed = embedding.fit_transform(X[:100])\n",
    ">>> X_transformed.shape\n",
    "(100, 2)\n",
    "``` "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
